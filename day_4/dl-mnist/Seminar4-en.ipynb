{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano, Lasagne\n",
    "and why they matter\n",
    "\n",
    "\n",
    "### got no lasagne?\n",
    "Install the __bleeding edge__ version from here: http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    res = []\n",
    "    for i in range(N+1):\n",
    "        res.append(i**2)\n",
    "    return res\n",
    "print sum_squares(10)\n",
    "\n",
    "def sum_squares_b(N):\n",
    "    return (np.arange(N) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 s, sys: 8.82 s, total: 23 s\n",
      "Wall time: 23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 25,\n",
       " 36,\n",
       " 49,\n",
       " 64,\n",
       " 81,\n",
       " 100,\n",
       " 121,\n",
       " 144,\n",
       " 169,\n",
       " 196,\n",
       " 225,\n",
       " 256,\n",
       " 289,\n",
       " 324,\n",
       " 361,\n",
       " 400,\n",
       " 441,\n",
       " 484,\n",
       " 529,\n",
       " 576,\n",
       " 625,\n",
       " 676,\n",
       " 729,\n",
       " 784,\n",
       " 841,\n",
       " 900,\n",
       " 961,\n",
       " 1024,\n",
       " 1089,\n",
       " 1156,\n",
       " 1225,\n",
       " 1296,\n",
       " 1369,\n",
       " 1444,\n",
       " 1521,\n",
       " 1600,\n",
       " 1681,\n",
       " 1764,\n",
       " 1849,\n",
       " 1936,\n",
       " 2025,\n",
       " 2116,\n",
       " 2209,\n",
       " 2304,\n",
       " 2401,\n",
       " 2500,\n",
       " 2601,\n",
       " 2704,\n",
       " 2809,\n",
       " 2916,\n",
       " 3025,\n",
       " 3136,\n",
       " 3249,\n",
       " 3364,\n",
       " 3481,\n",
       " 3600,\n",
       " 3721,\n",
       " 3844,\n",
       " 3969,\n",
       " 4096,\n",
       " 4225,\n",
       " 4356,\n",
       " 4489,\n",
       " 4624,\n",
       " 4761,\n",
       " 4900,\n",
       " 5041,\n",
       " 5184,\n",
       " 5329,\n",
       " 5476,\n",
       " 5625,\n",
       " 5776,\n",
       " 5929,\n",
       " 6084,\n",
       " 6241,\n",
       " 6400,\n",
       " 6561,\n",
       " 6724,\n",
       " 6889,\n",
       " 7056,\n",
       " 7225,\n",
       " 7396,\n",
       " 7569,\n",
       " 7744,\n",
       " 7921,\n",
       " 8100,\n",
       " 8281,\n",
       " 8464,\n",
       " 8649,\n",
       " 8836,\n",
       " 9025,\n",
       " 9216,\n",
       " 9409,\n",
       " 9604,\n",
       " 9801,\n",
       " 10000,\n",
       " 10201,\n",
       " 10404,\n",
       " 10609,\n",
       " 10816,\n",
       " 11025,\n",
       " 11236,\n",
       " 11449,\n",
       " 11664,\n",
       " 11881,\n",
       " 12100,\n",
       " 12321,\n",
       " 12544,\n",
       " 12769,\n",
       " 12996,\n",
       " 13225,\n",
       " 13456,\n",
       " 13689,\n",
       " 13924,\n",
       " 14161,\n",
       " 14400,\n",
       " 14641,\n",
       " 14884,\n",
       " 15129,\n",
       " 15376,\n",
       " 15625,\n",
       " 15876,\n",
       " 16129,\n",
       " 16384,\n",
       " 16641,\n",
       " 16900,\n",
       " 17161,\n",
       " 17424,\n",
       " 17689,\n",
       " 17956,\n",
       " 18225,\n",
       " 18496,\n",
       " 18769,\n",
       " 19044,\n",
       " 19321,\n",
       " 19600,\n",
       " 19881,\n",
       " 20164,\n",
       " 20449,\n",
       " 20736,\n",
       " 21025,\n",
       " 21316,\n",
       " 21609,\n",
       " 21904,\n",
       " 22201,\n",
       " 22500,\n",
       " 22801,\n",
       " 23104,\n",
       " 23409,\n",
       " 23716,\n",
       " 24025,\n",
       " 24336,\n",
       " 24649,\n",
       " 24964,\n",
       " 25281,\n",
       " 25600,\n",
       " 25921,\n",
       " 26244,\n",
       " 26569,\n",
       " 26896,\n",
       " 27225,\n",
       " 27556,\n",
       " 27889,\n",
       " 28224,\n",
       " 28561,\n",
       " 28900,\n",
       " 29241,\n",
       " 29584,\n",
       " 29929,\n",
       " 30276,\n",
       " 30625,\n",
       " 30976,\n",
       " 31329,\n",
       " 31684,\n",
       " 32041,\n",
       " 32400,\n",
       " 32761,\n",
       " 33124,\n",
       " 33489,\n",
       " 33856,\n",
       " 34225,\n",
       " 34596,\n",
       " 34969,\n",
       " 35344,\n",
       " 35721,\n",
       " 36100,\n",
       " 36481,\n",
       " 36864,\n",
       " 37249,\n",
       " 37636,\n",
       " 38025,\n",
       " 38416,\n",
       " 38809,\n",
       " 39204,\n",
       " 39601,\n",
       " 40000,\n",
       " 40401,\n",
       " 40804,\n",
       " 41209,\n",
       " 41616,\n",
       " 42025,\n",
       " 42436,\n",
       " 42849,\n",
       " 43264,\n",
       " 43681,\n",
       " 44100,\n",
       " 44521,\n",
       " 44944,\n",
       " 45369,\n",
       " 45796,\n",
       " 46225,\n",
       " 46656,\n",
       " 47089,\n",
       " 47524,\n",
       " 47961,\n",
       " 48400,\n",
       " 48841,\n",
       " 49284,\n",
       " 49729,\n",
       " 50176,\n",
       " 50625,\n",
       " 51076,\n",
       " 51529,\n",
       " 51984,\n",
       " 52441,\n",
       " 52900,\n",
       " 53361,\n",
       " 53824,\n",
       " 54289,\n",
       " 54756,\n",
       " 55225,\n",
       " 55696,\n",
       " 56169,\n",
       " 56644,\n",
       " 57121,\n",
       " 57600,\n",
       " 58081,\n",
       " 58564,\n",
       " 59049,\n",
       " 59536,\n",
       " 60025,\n",
       " 60516,\n",
       " 61009,\n",
       " 61504,\n",
       " 62001,\n",
       " 62500,\n",
       " 63001,\n",
       " 63504,\n",
       " 64009,\n",
       " 64516,\n",
       " 65025,\n",
       " 65536,\n",
       " 66049,\n",
       " 66564,\n",
       " 67081,\n",
       " 67600,\n",
       " 68121,\n",
       " 68644,\n",
       " 69169,\n",
       " 69696,\n",
       " 70225,\n",
       " 70756,\n",
       " 71289,\n",
       " 71824,\n",
       " 72361,\n",
       " 72900,\n",
       " 73441,\n",
       " 73984,\n",
       " 74529,\n",
       " 75076,\n",
       " 75625,\n",
       " 76176,\n",
       " 76729,\n",
       " 77284,\n",
       " 77841,\n",
       " 78400,\n",
       " 78961,\n",
       " 79524,\n",
       " 80089,\n",
       " 80656,\n",
       " 81225,\n",
       " 81796,\n",
       " 82369,\n",
       " 82944,\n",
       " 83521,\n",
       " 84100,\n",
       " 84681,\n",
       " 85264,\n",
       " 85849,\n",
       " 86436,\n",
       " 87025,\n",
       " 87616,\n",
       " 88209,\n",
       " 88804,\n",
       " 89401,\n",
       " 90000,\n",
       " 90601,\n",
       " 91204,\n",
       " 91809,\n",
       " 92416,\n",
       " 93025,\n",
       " 93636,\n",
       " 94249,\n",
       " 94864,\n",
       " 95481,\n",
       " 96100,\n",
       " 96721,\n",
       " 97344,\n",
       " 97969,\n",
       " 98596,\n",
       " 99225,\n",
       " 99856,\n",
       " 100489,\n",
       " 101124,\n",
       " 101761,\n",
       " 102400,\n",
       " 103041,\n",
       " 103684,\n",
       " 104329,\n",
       " 104976,\n",
       " 105625,\n",
       " 106276,\n",
       " 106929,\n",
       " 107584,\n",
       " 108241,\n",
       " 108900,\n",
       " 109561,\n",
       " 110224,\n",
       " 110889,\n",
       " 111556,\n",
       " 112225,\n",
       " 112896,\n",
       " 113569,\n",
       " 114244,\n",
       " 114921,\n",
       " 115600,\n",
       " 116281,\n",
       " 116964,\n",
       " 117649,\n",
       " 118336,\n",
       " 119025,\n",
       " 119716,\n",
       " 120409,\n",
       " 121104,\n",
       " 121801,\n",
       " 122500,\n",
       " 123201,\n",
       " 123904,\n",
       " 124609,\n",
       " 125316,\n",
       " 126025,\n",
       " 126736,\n",
       " 127449,\n",
       " 128164,\n",
       " 128881,\n",
       " 129600,\n",
       " 130321,\n",
       " 131044,\n",
       " 131769,\n",
       " 132496,\n",
       " 133225,\n",
       " 133956,\n",
       " 134689,\n",
       " 135424,\n",
       " 136161,\n",
       " 136900,\n",
       " 137641,\n",
       " 138384,\n",
       " 139129,\n",
       " 139876,\n",
       " 140625,\n",
       " 141376,\n",
       " 142129,\n",
       " 142884,\n",
       " 143641,\n",
       " 144400,\n",
       " 145161,\n",
       " 145924,\n",
       " 146689,\n",
       " 147456,\n",
       " 148225,\n",
       " 148996,\n",
       " 149769,\n",
       " 150544,\n",
       " 151321,\n",
       " 152100,\n",
       " 152881,\n",
       " 153664,\n",
       " 154449,\n",
       " 155236,\n",
       " 156025,\n",
       " 156816,\n",
       " 157609,\n",
       " 158404,\n",
       " 159201,\n",
       " 160000,\n",
       " 160801,\n",
       " 161604,\n",
       " 162409,\n",
       " 163216,\n",
       " 164025,\n",
       " 164836,\n",
       " 165649,\n",
       " 166464,\n",
       " 167281,\n",
       " 168100,\n",
       " 168921,\n",
       " 169744,\n",
       " 170569,\n",
       " 171396,\n",
       " 172225,\n",
       " 173056,\n",
       " 173889,\n",
       " 174724,\n",
       " 175561,\n",
       " 176400,\n",
       " 177241,\n",
       " 178084,\n",
       " 178929,\n",
       " 179776,\n",
       " 180625,\n",
       " 181476,\n",
       " 182329,\n",
       " 183184,\n",
       " 184041,\n",
       " 184900,\n",
       " 185761,\n",
       " 186624,\n",
       " 187489,\n",
       " 188356,\n",
       " 189225,\n",
       " 190096,\n",
       " 190969,\n",
       " 191844,\n",
       " 192721,\n",
       " 193600,\n",
       " 194481,\n",
       " 195364,\n",
       " 196249,\n",
       " 197136,\n",
       " 198025,\n",
       " 198916,\n",
       " 199809,\n",
       " 200704,\n",
       " 201601,\n",
       " 202500,\n",
       " 203401,\n",
       " 204304,\n",
       " 205209,\n",
       " 206116,\n",
       " 207025,\n",
       " 207936,\n",
       " 208849,\n",
       " 209764,\n",
       " 210681,\n",
       " 211600,\n",
       " 212521,\n",
       " 213444,\n",
       " 214369,\n",
       " 215296,\n",
       " 216225,\n",
       " 217156,\n",
       " 218089,\n",
       " 219024,\n",
       " 219961,\n",
       " 220900,\n",
       " 221841,\n",
       " 222784,\n",
       " 223729,\n",
       " 224676,\n",
       " 225625,\n",
       " 226576,\n",
       " 227529,\n",
       " 228484,\n",
       " 229441,\n",
       " 230400,\n",
       " 231361,\n",
       " 232324,\n",
       " 233289,\n",
       " 234256,\n",
       " 235225,\n",
       " 236196,\n",
       " 237169,\n",
       " 238144,\n",
       " 239121,\n",
       " 240100,\n",
       " 241081,\n",
       " 242064,\n",
       " 243049,\n",
       " 244036,\n",
       " 245025,\n",
       " 246016,\n",
       " 247009,\n",
       " 248004,\n",
       " 249001,\n",
       " 250000,\n",
       " 251001,\n",
       " 252004,\n",
       " 253009,\n",
       " 254016,\n",
       " 255025,\n",
       " 256036,\n",
       " 257049,\n",
       " 258064,\n",
       " 259081,\n",
       " 260100,\n",
       " 261121,\n",
       " 262144,\n",
       " 263169,\n",
       " 264196,\n",
       " 265225,\n",
       " 266256,\n",
       " 267289,\n",
       " 268324,\n",
       " 269361,\n",
       " 270400,\n",
       " 271441,\n",
       " 272484,\n",
       " 273529,\n",
       " 274576,\n",
       " 275625,\n",
       " 276676,\n",
       " 277729,\n",
       " 278784,\n",
       " 279841,\n",
       " 280900,\n",
       " 281961,\n",
       " 283024,\n",
       " 284089,\n",
       " 285156,\n",
       " 286225,\n",
       " 287296,\n",
       " 288369,\n",
       " 289444,\n",
       " 290521,\n",
       " 291600,\n",
       " 292681,\n",
       " 293764,\n",
       " 294849,\n",
       " 295936,\n",
       " 297025,\n",
       " 298116,\n",
       " 299209,\n",
       " 300304,\n",
       " 301401,\n",
       " 302500,\n",
       " 303601,\n",
       " 304704,\n",
       " 305809,\n",
       " 306916,\n",
       " 308025,\n",
       " 309136,\n",
       " 310249,\n",
       " 311364,\n",
       " 312481,\n",
       " 313600,\n",
       " 314721,\n",
       " 315844,\n",
       " 316969,\n",
       " 318096,\n",
       " 319225,\n",
       " 320356,\n",
       " 321489,\n",
       " 322624,\n",
       " 323761,\n",
       " 324900,\n",
       " 326041,\n",
       " 327184,\n",
       " 328329,\n",
       " 329476,\n",
       " 330625,\n",
       " 331776,\n",
       " 332929,\n",
       " 334084,\n",
       " 335241,\n",
       " 336400,\n",
       " 337561,\n",
       " 338724,\n",
       " 339889,\n",
       " 341056,\n",
       " 342225,\n",
       " 343396,\n",
       " 344569,\n",
       " 345744,\n",
       " 346921,\n",
       " 348100,\n",
       " 349281,\n",
       " 350464,\n",
       " 351649,\n",
       " 352836,\n",
       " 354025,\n",
       " 355216,\n",
       " 356409,\n",
       " 357604,\n",
       " 358801,\n",
       " 360000,\n",
       " 361201,\n",
       " 362404,\n",
       " 363609,\n",
       " 364816,\n",
       " 366025,\n",
       " 367236,\n",
       " 368449,\n",
       " 369664,\n",
       " 370881,\n",
       " 372100,\n",
       " 373321,\n",
       " 374544,\n",
       " 375769,\n",
       " 376996,\n",
       " 378225,\n",
       " 379456,\n",
       " 380689,\n",
       " 381924,\n",
       " 383161,\n",
       " 384400,\n",
       " 385641,\n",
       " 386884,\n",
       " 388129,\n",
       " 389376,\n",
       " 390625,\n",
       " 391876,\n",
       " 393129,\n",
       " 394384,\n",
       " 395641,\n",
       " 396900,\n",
       " 398161,\n",
       " 399424,\n",
       " 400689,\n",
       " 401956,\n",
       " 403225,\n",
       " 404496,\n",
       " 405769,\n",
       " 407044,\n",
       " 408321,\n",
       " 409600,\n",
       " 410881,\n",
       " 412164,\n",
       " 413449,\n",
       " 414736,\n",
       " 416025,\n",
       " 417316,\n",
       " 418609,\n",
       " 419904,\n",
       " 421201,\n",
       " 422500,\n",
       " 423801,\n",
       " 425104,\n",
       " 426409,\n",
       " 427716,\n",
       " 429025,\n",
       " 430336,\n",
       " 431649,\n",
       " 432964,\n",
       " 434281,\n",
       " 435600,\n",
       " 436921,\n",
       " 438244,\n",
       " 439569,\n",
       " 440896,\n",
       " 442225,\n",
       " 443556,\n",
       " 444889,\n",
       " 446224,\n",
       " 447561,\n",
       " 448900,\n",
       " 450241,\n",
       " 451584,\n",
       " 452929,\n",
       " 454276,\n",
       " 455625,\n",
       " 456976,\n",
       " 458329,\n",
       " 459684,\n",
       " 461041,\n",
       " 462400,\n",
       " 463761,\n",
       " 465124,\n",
       " 466489,\n",
       " 467856,\n",
       " 469225,\n",
       " 470596,\n",
       " 471969,\n",
       " 473344,\n",
       " 474721,\n",
       " 476100,\n",
       " 477481,\n",
       " 478864,\n",
       " 480249,\n",
       " 481636,\n",
       " 483025,\n",
       " 484416,\n",
       " 485809,\n",
       " 487204,\n",
       " 488601,\n",
       " 490000,\n",
       " 491401,\n",
       " 492804,\n",
       " 494209,\n",
       " 495616,\n",
       " 497025,\n",
       " 498436,\n",
       " 499849,\n",
       " 501264,\n",
       " 502681,\n",
       " 504100,\n",
       " 505521,\n",
       " 506944,\n",
       " 508369,\n",
       " 509796,\n",
       " 511225,\n",
       " 512656,\n",
       " 514089,\n",
       " 515524,\n",
       " 516961,\n",
       " 518400,\n",
       " 519841,\n",
       " 521284,\n",
       " 522729,\n",
       " 524176,\n",
       " 525625,\n",
       " 527076,\n",
       " 528529,\n",
       " 529984,\n",
       " 531441,\n",
       " 532900,\n",
       " 534361,\n",
       " 535824,\n",
       " 537289,\n",
       " 538756,\n",
       " 540225,\n",
       " 541696,\n",
       " 543169,\n",
       " 544644,\n",
       " 546121,\n",
       " 547600,\n",
       " 549081,\n",
       " 550564,\n",
       " 552049,\n",
       " 553536,\n",
       " 555025,\n",
       " 556516,\n",
       " 558009,\n",
       " 559504,\n",
       " 561001,\n",
       " 562500,\n",
       " 564001,\n",
       " 565504,\n",
       " 567009,\n",
       " 568516,\n",
       " 570025,\n",
       " 571536,\n",
       " 573049,\n",
       " 574564,\n",
       " 576081,\n",
       " 577600,\n",
       " 579121,\n",
       " 580644,\n",
       " 582169,\n",
       " 583696,\n",
       " 585225,\n",
       " 586756,\n",
       " 588289,\n",
       " 589824,\n",
       " 591361,\n",
       " 592900,\n",
       " 594441,\n",
       " 595984,\n",
       " 597529,\n",
       " 599076,\n",
       " 600625,\n",
       " 602176,\n",
       " 603729,\n",
       " 605284,\n",
       " 606841,\n",
       " 608400,\n",
       " 609961,\n",
       " 611524,\n",
       " 613089,\n",
       " 614656,\n",
       " 616225,\n",
       " 617796,\n",
       " 619369,\n",
       " 620944,\n",
       " 622521,\n",
       " 624100,\n",
       " 625681,\n",
       " 627264,\n",
       " 628849,\n",
       " 630436,\n",
       " 632025,\n",
       " 633616,\n",
       " 635209,\n",
       " 636804,\n",
       " 638401,\n",
       " 640000,\n",
       " 641601,\n",
       " 643204,\n",
       " 644809,\n",
       " 646416,\n",
       " 648025,\n",
       " 649636,\n",
       " 651249,\n",
       " 652864,\n",
       " 654481,\n",
       " 656100,\n",
       " 657721,\n",
       " 659344,\n",
       " 660969,\n",
       " 662596,\n",
       " 664225,\n",
       " 665856,\n",
       " 667489,\n",
       " 669124,\n",
       " 670761,\n",
       " 672400,\n",
       " 674041,\n",
       " 675684,\n",
       " 677329,\n",
       " 678976,\n",
       " 680625,\n",
       " 682276,\n",
       " 683929,\n",
       " 685584,\n",
       " 687241,\n",
       " 688900,\n",
       " 690561,\n",
       " 692224,\n",
       " 693889,\n",
       " 695556,\n",
       " 697225,\n",
       " 698896,\n",
       " 700569,\n",
       " 702244,\n",
       " 703921,\n",
       " 705600,\n",
       " 707281,\n",
       " 708964,\n",
       " 710649,\n",
       " 712336,\n",
       " 714025,\n",
       " 715716,\n",
       " 717409,\n",
       " 719104,\n",
       " 720801,\n",
       " 722500,\n",
       " 724201,\n",
       " 725904,\n",
       " 727609,\n",
       " 729316,\n",
       " 731025,\n",
       " 732736,\n",
       " 734449,\n",
       " 736164,\n",
       " 737881,\n",
       " 739600,\n",
       " 741321,\n",
       " 743044,\n",
       " 744769,\n",
       " 746496,\n",
       " 748225,\n",
       " 749956,\n",
       " 751689,\n",
       " 753424,\n",
       " 755161,\n",
       " 756900,\n",
       " 758641,\n",
       " 760384,\n",
       " 762129,\n",
       " 763876,\n",
       " 765625,\n",
       " 767376,\n",
       " 769129,\n",
       " 770884,\n",
       " 772641,\n",
       " 774400,\n",
       " 776161,\n",
       " 777924,\n",
       " 779689,\n",
       " 781456,\n",
       " 783225,\n",
       " 784996,\n",
       " 786769,\n",
       " 788544,\n",
       " 790321,\n",
       " 792100,\n",
       " 793881,\n",
       " 795664,\n",
       " 797449,\n",
       " 799236,\n",
       " 801025,\n",
       " 802816,\n",
       " 804609,\n",
       " 806404,\n",
       " 808201,\n",
       " 810000,\n",
       " 811801,\n",
       " 813604,\n",
       " 815409,\n",
       " 817216,\n",
       " 819025,\n",
       " 820836,\n",
       " 822649,\n",
       " 824464,\n",
       " 826281,\n",
       " 828100,\n",
       " 829921,\n",
       " 831744,\n",
       " 833569,\n",
       " 835396,\n",
       " 837225,\n",
       " 839056,\n",
       " 840889,\n",
       " 842724,\n",
       " 844561,\n",
       " 846400,\n",
       " 848241,\n",
       " 850084,\n",
       " 851929,\n",
       " 853776,\n",
       " 855625,\n",
       " 857476,\n",
       " 859329,\n",
       " 861184,\n",
       " 863041,\n",
       " 864900,\n",
       " 866761,\n",
       " 868624,\n",
       " 870489,\n",
       " 872356,\n",
       " 874225,\n",
       " 876096,\n",
       " 877969,\n",
       " 879844,\n",
       " 881721,\n",
       " 883600,\n",
       " 885481,\n",
       " 887364,\n",
       " 889249,\n",
       " 891136,\n",
       " 893025,\n",
       " 894916,\n",
       " 896809,\n",
       " 898704,\n",
       " 900601,\n",
       " 902500,\n",
       " 904401,\n",
       " 906304,\n",
       " 908209,\n",
       " 910116,\n",
       " 912025,\n",
       " 913936,\n",
       " 915849,\n",
       " 917764,\n",
       " 919681,\n",
       " 921600,\n",
       " 923521,\n",
       " 925444,\n",
       " 927369,\n",
       " 929296,\n",
       " 931225,\n",
       " 933156,\n",
       " 935089,\n",
       " 937024,\n",
       " 938961,\n",
       " 940900,\n",
       " 942841,\n",
       " 944784,\n",
       " 946729,\n",
       " 948676,\n",
       " 950625,\n",
       " 952576,\n",
       " 954529,\n",
       " 956484,\n",
       " 958441,\n",
       " 960400,\n",
       " 962361,\n",
       " 964324,\n",
       " 966289,\n",
       " 968256,\n",
       " 970225,\n",
       " 972196,\n",
       " 974169,\n",
       " 976144,\n",
       " 978121,\n",
       " 980100,\n",
       " 982081,\n",
       " 984064,\n",
       " 986049,\n",
       " 988036,\n",
       " 990025,\n",
       " 992016,\n",
       " 994009,\n",
       " 996004,\n",
       " 998001,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#I gonna be function parameter\n",
    "N = T.scalar(\"a dimension\",dtype='int32')\n",
    "\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#Compiling the recipe of computing \"result\" given N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 350 ms, sys: 60.8 ms, total: 411 ms\n",
      "Wall time: 409 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(662921401752298880)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "* 1 You define inputs f your future function;\n",
    "* 2 You write a recipe for some transformation of inputs;\n",
    "* 3 You compile it;\n",
    "* You have just got a function!\n",
    "* The gobbledegooky version: you define a function as symbolic computation graph.\n",
    "\n",
    "\n",
    "* There are two main kinвs of entities: \"Inputs\" and \"Transformations\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be integers, floats of booleans (uint8) of various size.\n",
    "\n",
    "\n",
    "* An input is a placeholder for function parameters.\n",
    " * N from example above\n",
    "\n",
    "\n",
    "* Transformations are the recipes for computing something given inputs and transformation\n",
    " * (T.arange(N)^2).sum() are 3 sequential transformations of N\n",
    " * Doubles all functions of numpy vector syntax\n",
    " * You can almost always go with replacing \"np.function\" with \"T.function\" aka \"theano.tensor.function\"\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * and so on.\n",
    "   * builtin operations also work that way\n",
    "   * np.arange(10).mean() -> T.arange(10).mean()\n",
    "   * Once upon a blue moon the functions have different names or locations (e.g. T.extra_ops)\n",
    "     * Ask us or google it\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "example_input_integer = T.scalar(\"scalar input\",dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4(\"four dimensional tensor input\") #dtype = theano.config.floatX by default\n",
    "#не бойся, тензор нам не пригодится\n",
    "\n",
    "\n",
    "\n",
    "input_vector = T.vector(\"\", dtype='int32') # vector of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "#transofrmation: elementwise multiplication\n",
    "double_the_vector = input_vector*2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector**2 - input_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,4,1,0\n"
     ]
    }
   ],
   "source": [
    "#Practice time:\n",
    "#create two vectors of size float32\n",
    "my_vector = T.vector(\"1,2,3,4\", dtype='int32')#student.init_float32_vector()\n",
    "my_vector2 = T.vector(\"2,4,1,0\", dtype='int32')#student.init_one_more_such_vector()\n",
    "print my_vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = abs(my_vector**2 + my_vector2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{abs_,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "print my_transformation\n",
    "#it's okay it aint a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling\n",
    "* So far we were using \"symbolic\" variables and transformations\n",
    " * Defining the recipe for computation, but not computing anything\n",
    "* To use the recipe, one should compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [my_vector,my_vector2]\n",
    "outputs = [my_transformation]\n",
    "\n",
    "# The next lines compile a function that takes two vectors and computes your transformation\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using python lists:\n",
      "[array([17, 29, 45], dtype=int32)]\n",
      "\n",
      "using numpy arrays:\n",
      "[array([ 25,  26,  29,  34,  41,  50,  61,  74,  89, 117], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "#using function with, lists:\n",
    "print \"using python lists:\"\n",
    "print my_function([1,2,3],[4,5,6])\n",
    "print\n",
    "\n",
    "#Or using numpy arrays:\n",
    "#btw, that 'float' dtype is casted to secong parameter dtype which is float32\n",
    "print \"using numpy arrays:\"\n",
    "print my_function(np.arange(10),\n",
    "                  np.linspace(5,6,10,dtype='float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "* Compilation can take a while for big functions\n",
    "* To avoid waiting, one can evaluate transformations without compiling\n",
    "* Without compilation, the code runs slower, so consider reducing input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 29 45]\n",
      "add 2 vectors [5 7 9]\n",
      "vector's shape: [3]\n"
     ]
    }
   ],
   "source": [
    "#a dictionary of inputs\n",
    "my_function_inputs = {\n",
    "    my_vector:[1,2,3],\n",
    "    my_vector2:[4,5,6]\n",
    "}\n",
    "\n",
    "# evaluate my_transformation\n",
    "# has to match with compiled function output\n",
    "print my_transformation.eval(my_function_inputs)\n",
    "\n",
    "\n",
    "# can compute transformations on the fly\n",
    "print \"add 2 vectors\", (my_vector + my_vector2).eval(my_function_inputs)\n",
    "\n",
    "#!WARNING! if your transformation only depends on some inputs,\n",
    "#do not provide the rest of them\n",
    "print \"vector's shape:\", my_vector.shape.eval({\n",
    "        my_vector:[1,2,3]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для отладки желательно уменьшить масштаб задачи. Если вы планировали послать на вход вектор из 10^9 примеров, пошлите 10~100.\n",
    "* Если #ОЧЕНЬ нужно послать большой вектор, быстрее скомпилировать функцию обычным способом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "Y = T.vector(dtype='float32')\n",
    "Y_pred = T.vector(dtype='float32')\n",
    "\n",
    "mse = ((Y- Y_pred)**2).mean()\n",
    "\n",
    "compute_mse = theano.function([Y,Y_pred],mse,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_mse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fd26d7e90710>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mel_2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mtrue_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mel_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mmy_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mel_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_mse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m'Wrong result:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_mse' is not defined"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el,el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print 'Wrong result:'\n",
    "                print 'mse(%s,%s)'%(el,el_2)\n",
    "                print \"should be: %f, but your function returned %f\"%(true_mse,my_mse)\n",
    "                raise ValueError,\"Что-то не так\"\n",
    "\n",
    "print \"All tests passed\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared variables\n",
    "\n",
    "* The inputs and transformations only exist when function is called\n",
    "\n",
    "* Shared variables always stay in memory like global variables\n",
    " * Shared variables can be included into a symbolic graph\n",
    " * They can be set and evaluated using special methods\n",
    "   * but they can't change value arbitrarily during symbolic graph computation\n",
    "   * we'll cover that later;\n",
    " \n",
    " \n",
    "* Hint: such variables are a perfect place to store network parameters\n",
    " * e.g. weights or some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = theano.shared(np.ones(10,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print \"initial value\",shared_vector_1.get_value()\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting new value\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#getting that new value\n",
    "print \"new value\", shared_vector_1.get_value()\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a recipe (transformation) that computes an elementwise transformation of shared_vector and input_scalar\n",
    "#Compile as a function of input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = <student.write_recipe()>\n",
    "\n",
    "\n",
    "shared_times_n = <student.compile_function()>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changing value of vector 1 (output should change)\n",
    "shared_vector_1.set_value([-1,0,1])\n",
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T.grad - why theano matters\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ transformation over one or several scalar or vector (or tensor) transformations or inputs.\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared = map(fun,x)\n",
    "x_squared_der = map(grad,x)\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = <student.compute_grad_over_scalar_and_vector()>\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - Updates\n",
    "\n",
    "* updates are a way of changing shared variables at after function call.\n",
    "\n",
    "* technically it's a dictionary {shared_variable : a recipe for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiply shared vector by a number and save the product back into shared vector\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #return vector times scalar\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #and write this same result bach into shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#initial shared_vector_1\n",
    "print \"initial shared value:\" ,shared_vector_1.get_value()\n",
    "\n",
    "# evaluating the function (shared_vector_1 will be changed)\n",
    "print \"compute_and_save(2) returns\",compute_and_save(2)\n",
    "\n",
    "#evaluate new shared_vector_1\n",
    "print \"new shared value:\" ,shared_vector_1.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X,y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    "$L(target,pred proba) =  mean( target * log(pred proba) + (1-target) * log(1-pred proba)) $\n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print \"y [shape - %s]:\"%(str(y.shape)),y[:10]\n",
    "\n",
    "print \"X [shape - %s]:\"%(str(X.shape))\n",
    "print X[:3]\n",
    "print y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = <student.code_me()>\n",
    "input_X = <student.code_me()>\n",
    "input_y = <student.code_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <predicted probabilities for input_X>\n",
    "loss = <logistic loss (scalar, mean over sample)>\n",
    "\n",
    "grad = <gradient of loss over model weights>\n",
    "\n",
    "\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <new weights after gradient step>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_function = <compile function that takes X and y, returns log loss and updates weights>\n",
    "predict_function = <compile function that takes X and computes probabilities of y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print \"loss at iter %i:%.4f\"%(i,loss_i)\n",
    "    print \"train auc:\",roc_auc_score(y_train,predict_function(X_train))\n",
    "    print \"test auc:\",roc_auc_score(y_test,predict_function(X_test))\n",
    "\n",
    "    \n",
    "print \"resulting weights:\"\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "#What do need to implement\n",
    "# 1) Shuffle data\n",
    "# - Gotta shuffle X and y the same way not to break the correspondence between X_i and y_i\n",
    "# 3) Split data into minibatches of batch_size\n",
    "# - If data size is not a multiple of batch_size, make one last batch smaller.\n",
    "# 4) return a list (or an iterator) of pairs\n",
    "# - (подгруппа картинок, ответы из y на эту подгруппу)\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    <return an iterable of (X_batch, y_batch)  batches of images and answers for them>\n",
    "    \n",
    "        \n",
    "        \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# You feel lost and wish you stayed home tonight?\n",
    "# Go search for a similar function at\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better network\n",
    "\n",
    "\n",
    "* The quest is to create a network that gets at least 99% at test set\n",
    " * In case you tried several architectures and have a __detailed__ report - 97.5% \"is fine too\". \n",
    " \n",
    "__ There is a mini-report at the end that you will have to fill in. We recommend to read it first and fill in while you are iterating. __\n",
    " \n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    "\n",
    " * Network size\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, \n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Better optimization - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Convolution layers\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU.\n",
    "     * If you are CPU-only, we still recomment to try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    " \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc\n",
    "   \n",
    " \n",
    " \n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "<student.code_neural_network_architecture()>\n",
    "\n",
    "dense_output = <your network output>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss = <loss function>\n",
    "\n",
    "#<optionally add regularization>\n",
    "\n",
    "accuracy = <mean accuracy score for evaluation> \n",
    "\n",
    "#weight updates\n",
    "updates = <try different update methods>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#итерации обучения\n",
    "\n",
    "num_epochs = <how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = <how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `___ ___`, and here's my story\n",
    "\n",
    "A long ago in a galaxy far far away, when it was still more than an hour before deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* brief text on what was\n",
    "* the original idea\n",
    "* and why it was so\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* Some explaination about what were the results,\n",
    "* what worked and what didn't\n",
    "* most importantly - what next steps were taken, if any\n",
    "* and what were their respective outcomes\n",
    "\n",
    "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
    "* what was the final architecture\n",
    "* as well as training method and tricks\n",
    "\n",
    "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: __\n",
    "* accuracy on validation: __\n",
    "* accuracy on test: __\n",
    "\n",
    "\n",
    "[an optional afterword and mortal curses on assignment authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
