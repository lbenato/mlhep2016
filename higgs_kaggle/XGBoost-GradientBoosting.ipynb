{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "In this notebook we prepare a simple solution for the [kaggle challenge on higgs.](https://inclass.kaggle.com/c/mlhep-2016-higgs-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import root_numpy\n",
    "data = pandas.DataFrame(root_numpy.root2array('datasets/public_train_10000.root'))\n",
    "data = data[:100]\n",
    "test = pandas.DataFrame(root_numpy.root2array('datasets/public_test.root'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(set(data.columns) - {'event_id', 'target'})\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare high-level features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#high_level_features = ['m_jj', 'm_jjj', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'm_lv']\n",
    "#high_level_features_a = ['m_jj', 'm_jjj', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'm_lv', 'lepton_pt','mem_pt']\n",
    "#high_level_features_b = ['m_jj', 'm_jjj', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'lepton_pt','mem_pt','jet1_pt','jet2_pt','jet3_pt','jet4_pt']\n",
    "#high_level_features_c = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'lepton_pt','mem_pt','jet1_pt','jet2_pt','jet3_pt','jet4_pt']\n",
    "#high_level_features_d = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'lepton_pt','mem_pt','jet1_pt','jet2_pt','jet3_pt','jet4_pt','j_HT','phi_bb']\n",
    "#high_level_features_e = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'lepton_pt','mem_pt','jet1_pt','jet2_pt','jet3_pt','jet4_pt','j_HT','phi_bb','LT']\n",
    "data['j_HT'] = data['jet1_pt'] + data['jet2_pt'] + data['jet3_pt'] + data['jet4_pt']\n",
    "test['j_HT'] = test['jet1_pt'] + test['jet2_pt'] + test['jet3_pt'] + test['jet4_pt']\n",
    "data['phi_bb'] = data['jet1_phi'] + data['jet2_phi'] + data['jet3_phi'] + data['jet4_phi']\n",
    "test['phi_bb'] = test['jet1_phi'] + test['jet2_phi'] + test['jet3_phi'] + test['jet4_phi']\n",
    "data['LT'] = data['lepton_pt'] + data['mem_pt']\n",
    "test['LT'] = test['lepton_pt'] + test['mem_pt']\n",
    "high_level_features = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_wwbb', 'm_bb', 'm_wbb', 'jet1_pt','jet2_pt','jet3_pt','jet4_pt','j_HT','phi_bb','lepton_pt','mem_pt','LT']#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide training data into 2 parts \n",
    "`train_test_split` function is used to divide into 2 parts to preserve quality overestimating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split(data, random_state=11, train_size=0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.51190476190476186, 0.078064744336928574)\n"
     ]
    }
   ],
   "source": [
    "from rep.estimators import XGBoostClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "xgboost_cv = cross_val_score(XGBoostClassifier(max_depth = 6, min_child_weight = 1, gamma=0.3, colsample=1, nthreads=10),\n",
    "                             training_data[high_level_features].astype(np.float64),\n",
    "                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "print(xgboost_cv.mean(), xgboost_cv.std())\n",
    "\n",
    "#0.7668823025 senza opzioni con lepton_pt, mem_phi 100'000 eventi \n",
    "#0.72716605849516347 senza opzioni con lepton_pt, mem_phi 10'000 eventi\n",
    "#high level features a:\n",
    "#max_depth 5: 0.77373192433167481\n",
    "#max_depth 4: 0.7748102315375196, gamma=0.3: 0.7750851890287791\n",
    "#max_depth 6: 0.77178583194465911\n",
    "# high level features b:\n",
    "#max_depth 6, gamma=0.3 0.78655990568943845\n",
    "# high level features c:\n",
    "#max_depth 6, gamma=0.3 0.78747491255687363\n",
    "# high level features c with Ht and pt:\n",
    "#max_depth 6, gamma=0.3 0.78959440814072357\n",
    "# high level features c with Ht only:\n",
    "#max_depth 6, gamma=0.3 0.78882205872533173\n",
    "# high level features d:\n",
    "#max_depth 6, gamma=0.3 0.78911150552696641\n",
    "# high level features e:\n",
    "#max_depth 6, gamma=0.3 0.78987168613515957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from rep.estimators import XGBoostClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#xgboost_cv = cross_val_score(XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(xgboost_cv.mean(), xgboost_cv.std())\n",
    "\n",
    "\n",
    "#0.7668823025 senza opzioni con lepton_pt, mem_phi 100'000 eventi \n",
    "#0.72716605849516347 senza opzioni con lepton_pt, mem_phi 10'000 eventi\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "xgboost_grad_cv = cross_val_score(GradientBoostingClassifier(min_samples_split = 500, min_samples_leaf = 50, max_depth = 8, max_features = 'sqrt', subsample = 0.8),#(base_estimator=XGBoostClassifier(max_depth = 6, min_child_weight = 1, gamma=0., colsample=1, nthreads=10),n_estimators=10),\n",
    "                             training_data[high_level_features].astype(np.float64),\n",
    "                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "print(xgboost_grad_cv.mean(), xgboost_grad_cv.std())\n",
    "\n",
    "\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#grad_bag_cv = cross_val_score(BaggingClassifier(base_estimator=GradientBoostingClassifier(min_samples_split = 500, min_samples_leaf = 50, max_depth = 8, max_features = 'sqrt', subsample = 0.8),n_estimators=34),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(grad_bag_cv.mean(), grad_bag_cv.std())\n",
    "\n",
    "\n",
    "\n",
    "#boost_bagg = BaggingClassifier(base_estimator=XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),n_estimators=34, n_jobs=10)\n",
    "#boost_bagg.fit(training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64))\n",
    "\n",
    "#high level features a\n",
    "###0.78144373296283975 con max_depth = 5\n",
    "###0.77968141225306831 con max_depth = 4\n",
    "###0.7823899079878065 con max_depth = 6, gamma=0.3 0.78169084626057739\n",
    "# high level features b:\n",
    "#max_depth 6, gamma=0.3 0.7972091448484645\n",
    "#max_depth 6, gamma=0 0.79791828545435561\n",
    "# high level features c:\n",
    "#max_depth 6, gamma=0. 0.7992048882561511\n",
    "# high level features c with Ht an pt\n",
    "#max_depth 6, gamma=0.0.80147524606003273\n",
    "# high level features c with Ht only\n",
    "#max_depth 6, gamma=0.  0.79897399081169285\n",
    "#high level features d:\n",
    "#max_depth 6, gamma=0 0.80011445909587442\n",
    "#high level features d:\n",
    "#max_depth 6, gamma=0 0.80165832047956576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-325855086f37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m grad_bag_cv = cross_val_score(BaggingClassifier(base_estimator=GradientBoostingClassifier(min_samples_split = 500, min_samples_leaf = 50, max_depth = 8, max_features = 'sqrt', subsample = 0.8),n_estimators=34),\n\u001b[0;32m     21\u001b[0m                              \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhigh_level_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                              training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_bag_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_bag_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1431\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m                                               fit_params)\n\u001b[1;32m-> 1433\u001b[1;33m                       for train, test in cv)\n\u001b[0m\u001b[0;32m   1434\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 337\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/ensemble/bagging.pyc\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, verbose)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_indices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1025\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 784\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from rep.estimators import XGBoostClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#xgboost_cv = cross_val_score(XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(xgboost_cv.mean(), xgboost_cv.std())\n",
    "\n",
    "\n",
    "#0.7668823025 senza opzioni con lepton_pt, mem_phi 100'000 eventi \n",
    "#0.72716605849516347 senza opzioni con lepton_pt, mem_phi 10'000 eventi\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#xgboost_grad_cv = cross_val_score(GradientBoostingClassifier(min_samples_split = 500, min_samples_leaf = 50, max_depth = 8, max_features = 'sqrt', subsample = 0.8),#(base_estimator=XGBoostClassifier(max_depth = 6, min_child_weight = 1, gamma=0., colsample=1, nthreads=10),n_estimators=10),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(xgboost_grad_cv.mean(), xgboost_grad_cv.std())\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "grad_bag_cv = cross_val_score(BaggingClassifier(base_estimator=GradientBoostingClassifier(min_samples_split = 500, min_samples_leaf = 50, max_depth = 8, max_features = 'sqrt', subsample = 0.8),n_estimators=34),\n",
    "                             training_data[high_level_features].astype(np.float64),\n",
    "                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "print(grad_bag_cv.mean(), grad_bag_cv.std())\n",
    "\n",
    "\n",
    "\n",
    "#boost_bagg = BaggingClassifier(base_estimator=XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),n_estimators=34, n_jobs=10)\n",
    "#boost_bagg.fit(training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64))\n",
    "\n",
    "#high level features a\n",
    "###0.78144373296283975 con max_depth = 5\n",
    "###0.77968141225306831 con max_depth = 4\n",
    "###0.7823899079878065 con max_depth = 6, gamma=0.3 0.78169084626057739\n",
    "# high level features b:\n",
    "#max_depth 6, gamma=0.3 0.7972091448484645\n",
    "#max_depth 6, gamma=0 0.79791828545435561\n",
    "# high level features c:\n",
    "#max_depth 6, gamma=0. 0.7992048882561511\n",
    "# high level features c with Ht an pt\n",
    "#max_depth 6, gamma=0.0.80147524606003273\n",
    "# high level features c with Ht only\n",
    "#max_depth 6, gamma=0.  0.79897399081169285\n",
    "#high level features d:\n",
    "#max_depth 6, gamma=0 0.80011445909587442\n",
    "#high level features d:\n",
    "#max_depth 6, gamma=0 0.80165832047956576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=XGBoostClassifier(base_score=0.5, colsample=1, eta=0.3, features=None,\n",
       "         gamma=0, max_depth=6, min_child_weight=1, missing=-999.0,\n",
       "         n_estimators=10, nthreads=10, num_feature=None, random_state=0,\n",
       "         scale_pos_weight=1.0, subsample=1.0, verbose=0),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rep.estimators import XGBoostClassifier\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#xgboost_cv = cross_val_score(XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(xgboost_cv.mean(), xgboost_cv.std())\n",
    "\n",
    "\n",
    "#0.7668823025 senza opzioni con lepton_pt, mem_phi 100'000 eventi \n",
    "#0.72716605849516347 senza opzioni con lepton_pt, mem_phi 10'000 eventi\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#xgboost_bag_cv = cross_val_score(BaggingClassifier(base_estimator=XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0),n_estimators=34),\n",
    "#                             training_data[high_level_features].astype(np.float64),\n",
    "#                             training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "#print(xgboost_bag_cv.mean(), xgboost_bag_cv.std())\n",
    "boost_bagg = BaggingClassifier(base_estimator=XGBoostClassifier(max_depth = 6, min_child_weight = 1, gamma=0, \n",
    "                                                                colsample=1, n_estimators=10, nthreads=10))\n",
    "boost_bagg.fit(training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Risultati:\n",
    "#xgboost bag 25 n_estimator: 748619196\n",
    "#xgboost bag 30 n_estimator: 74810733\n",
    "#xgboost bag 35 n_estimator: 74842935567290336\n",
    "#xgboost bag 34 n_estimator: 74889629284010684\n",
    "#\"                      , max depth 5: 0.74948804173465056\n",
    "# \" \"                                , min child 1, gamma 0: 0.75022381545276473\n",
    "# + colsample=0.8: 0.74879395566741258\n",
    "# + colsample= 0.8 su 100'000: 0.77806206257685062\n",
    "# colsample=1 su 100'000 0.77841176129880307\n",
    "# max depths 6?\n",
    "#se tolgo mjjj, 30 estimators: peggio scende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rep.estimators import XGBoostClassifier\n",
    "xg = XGBoostClassifier(max_depth = 5, min_child_weight = 1, gamma=0, colsample=1)\n",
    "xg.fit(training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_on_xgb = cross_val_score(AdaBoostClassifier(base_estimator=XGBoostClassifier()),\n",
    "                              training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64), cv=4, scoring=\"roc_auc\")\n",
    "print(ada_on_xgb.mean(), ada_on_xgb.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xg_ada = AdaBoostClassifier(base_estimator=XGBoostClassifier())\n",
    "xg_ada.fit(training_data[high_level_features].astype(np.float64), training_data.target.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#0.71559484247046334 ada cross val: senza opzioni con lepton_pt, mem_phi 10'000 eventi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict validation sample (probability for each event)\n",
    "proba = xg.predict_proba(validation_data[high_level_features].astype(np.float64))\n",
    "#proba_ada = xg_ada.predict_proba(validation_data[high_level_features].astype(np.float64))\n",
    "#proba_boost_bag = boost_bagg.predict_proba(validation_data[high_level_features].astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict validation sample (probability for each event)\n",
    "#proba = xg.predict_proba(validation_data[high_level_features].astype(np.float64))\n",
    "#proba_ada = xg_ada.predict_proba(validation_data[high_level_features].astype(np.float64))\n",
    "proba_boost_bag = boost_bagg.predict_proba(validation_data[high_level_features].astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proba\n",
    "proba_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute quality (ROC AUC) on the validation set (to prevent overestimating quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take probability to be 1 class to compute ROC AUC\n",
    "#\n",
    "roc_auc_score(validation_data.target, proba[:, 1])\n",
    "#0.73411680585983141 con xgb da solo, senza opzioni, lep_pt, mem_phi, 10'000 eventi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take probability to be 1 class to compute ROC AUC\n",
    "#\n",
    "roc_auc_score(validation_data.target, proba_ada[:, 1])\n",
    "#0.7176807190099479 con xgb da solo, senza opzioni, lep_pt, mem_phi, 10'000 eventi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79228100410996594"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take probability to be 1 class to compute ROC AUC\n",
    "#\n",
    "roc_auc_score(validation_data.target, proba_boost_bag[:, 1])\n",
    "#0.7176807190099479 con xgb da solo, senza opzioni, lep_pt, mem_phi, 10'000 eventi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kaggle_proba = boost_bagg.predict_proba(test[high_level_features].astype(np.float64))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict test sample\n",
    "kaggle_proba = boost_bagg.predict_proba(test[high_level_features].astype(np.float64))[:, 1]\n",
    "#kaggle_proba = xg.predict_proba(test[high_level_features].astype(np.float64))[:, 1]\n",
    "kaggle_ids = test.event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='datasets/xg_bagg_tuned_setoffeatures_e.csv' target='_blank'>datasets/xg_bagg_tuned_setoffeatures_e.csv</a><br>"
      ],
      "text/plain": [
       "/notebooks/higgs_kaggle/datasets/xg_bagg_tuned_setoffeatures_e.csv"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "def create_solution(ids, proba, filename='xg_bagg_tuned_setoffeatures_e.csv'):\n",
    "    \"\"\"saves predictions to file and provides a link for downloading \"\"\"\n",
    "    pandas.DataFrame({'event_id': ids, 'prediction': proba}).to_csv('datasets/{}'.format(filename), index=False)\n",
    "    return FileLink('datasets/{}'.format(filename))\n",
    "    \n",
    "create_solution(kaggle_ids, kaggle_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -lah datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
